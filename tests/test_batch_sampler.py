# Copyright (c) 2023 Graphcore Ltd. All rights reserved.

from typing import Dict

import einops
import numpy as np
import pytest
from numpy.testing import assert_equal
from numpy.typing import NDArray

from besskge.batch_sampler import RandomShardedBatchSampler, RigidShardedBatchSampler
from besskge.dataset import KGDataset
from besskge.negative_sampler import RandomShardedNegativeSampler
from besskge.sharding import PartitionedTripleSet, Sharding

seed = 1234
n_entity = 500
n_relation_type = 10
n_shard = 4
n_triple = 2000
batches_per_step = 3
shard_bs = 120
n_negative = 250

np.random.seed(seed)

triples_h = np.random.randint(n_entity, size=(n_triple,))
triples_t = np.random.randint(n_entity, size=(n_triple,))
triples_r = np.random.randint(n_relation_type, size=(n_triple,))
triples = {"train": np.stack([triples_h, triples_r, triples_t], axis=1)}

ds = KGDataset(
    n_entity=n_entity,
    n_relation_type=n_relation_type,
    entity_dict=None,
    relation_dict=None,
    type_offsets=None,
    triples=triples,
    neg_heads=None,
    neg_tails=None,
)

sharding = Sharding.create(n_entity, n_shard, seed=seed)

ns = RandomShardedNegativeSampler(
    n_negative=n_negative,
    sharding=sharding,
    seed=seed,
    corruption_scheme="h",
    local_sampling=False,
    flat_negative_format=False,
)


def reconstruct_batch(
    batch: Dict[str, NDArray[np.int32]], partition_mode: str
) -> NDArray[np.int32]:
    """Un-shard the batch generated by a ShardedBatchSampler"""
    reconstructed_batch = np.empty(
        (n_shard, batches_per_step * shard_bs, 3), dtype=np.int32
    )
    for processing_shard in range(n_shard):
        heads = np.array([]).astype(np.int32)
        relations = np.array([]).astype(np.int32)
        tails = np.array([]).astype(np.int32)

        relations = np.concatenate(
            [relations, batch["relation"][:, processing_shard].flatten()]
        )

        if partition_mode == "h_shard":
            heads = np.concatenate(
                [
                    heads,
                    sharding.shard_and_idx_to_entity[
                        processing_shard, batch["head"][:, processing_shard]
                    ].flatten(),
                ]
            )
            # Batch already has global indices of tails
            tails = np.concatenate(
                [
                    tails,
                    batch["tail"][:, processing_shard].flatten(),
                ]
            )
        elif partition_mode == "t_shard":
            # Batch already has global indices of heads
            heads = np.concatenate(
                [
                    heads,
                    batch["head"][:, processing_shard].flatten(),
                ]
            )
            tails = np.concatenate(
                [
                    tails,
                    sharding.shard_and_idx_to_entity[
                        processing_shard, batch["tail"][:, processing_shard]
                    ].flatten(),
                ]
            )
        elif partition_mode == "ht_shardpair":
            heads = np.concatenate(
                [
                    heads,
                    sharding.shard_and_idx_to_entity[
                        processing_shard, batch["head"][:, processing_shard]
                    ].flatten(),
                ]
            )
            # Tails are sampled from all shards and sent to processing_shard
            # via all_to_all
            tails = np.concatenate(
                [
                    tails,
                    sharding.shard_and_idx_to_entity[
                        np.arange(n_shard)[None, :, None],
                        batch["tail"][:, :, processing_shard],
                    ].flatten(),
                ]
            )

        reconstructed_batch[processing_shard] = np.stack(
            [heads, relations, tails], axis=1
        )

    return reconstructed_batch


@pytest.mark.parametrize(
    "triple_partition_mode", ["h_shard", "t_shard", "ht_shardpair"]
)
@pytest.mark.parametrize("duplicate_batch", [True, False])
def test_random_bs(triple_partition_mode: str, duplicate_batch: bool) -> None:
    np.random.seed(seed)

    partitioned_triple_set = PartitionedTripleSet.create_from_dataset(
        ds, "train", sharding, partition_mode=triple_partition_mode
    )

    bs = RandomShardedBatchSampler(
        partitioned_triple_set=partitioned_triple_set,
        negative_sampler=ns,
        shard_bs=shard_bs,
        batches_per_step=batches_per_step,
        seed=seed,
        hrt_freq_weighting=False,
        duplicate_batch=duplicate_batch,
        return_triple_idx=True,
    )

    b = {k: v.numpy() for k, v in next(iter(bs.get_dataloader())).items()}
    rearrange_pattern = (
        "shard_h (step shard_t triple) hrt -> step shard_h shard_t triple hrt"
        if triple_partition_mode == "ht_shardpair"
        else "shard_h (step triple) hrt -> step shard_h triple hrt"
    )

    reconstructed_batch = einops.rearrange(
        reconstruct_batch(b, triple_partition_mode),
        rearrange_pattern,
        step=batches_per_step,
        shard_h=n_shard,
        triple=2 * bs.positive_per_partition
        if duplicate_batch
        else bs.positive_per_partition,
    )
    # Check that reconstructed triples are in dataset
    assert_equal(
        reconstructed_batch,
        ds.triples["train"][partitioned_triple_set.triple_sort_idx][b["triple_idx"]],
    )

    if duplicate_batch:
        cutpoint = b["head"].shape[-1] // 2
        for prop in ["head", "relation", "tail"]:
            assert_equal(b[prop][..., :cutpoint], b[prop][..., cutpoint:])


@pytest.mark.parametrize(
    "triple_partition_mode", ["h_shard", "t_shard", "ht_shardpair"]
)
@pytest.mark.parametrize("duplicate_batch", [True, False])
@pytest.mark.parametrize("shuffle", [True, False])
def test_rigid_bs(
    triple_partition_mode: str, duplicate_batch: bool, shuffle: bool
) -> None:
    np.random.seed(seed)

    partitioned_triple_set = PartitionedTripleSet.create_from_dataset(
        ds, "train", sharding, partition_mode=triple_partition_mode
    )

    bs = RigidShardedBatchSampler(
        partitioned_triple_set=partitioned_triple_set,
        negative_sampler=ns,
        shard_bs=shard_bs,
        batches_per_step=batches_per_step,
        seed=seed,
        hrt_freq_weighting=False,
        duplicate_batch=duplicate_batch,
    )

    sampler = bs.get_dataloader_sampler(shuffle=shuffle)
    # Reconstruct all triples seen in one epoch
    filtered_triples = []
    for idx in iter(sampler):
        b = {k: v.numpy() for k, v in bs[idx].items()}
        reconstructed_batch = reconstruct_batch(b, triple_partition_mode)
        # Discard padding triples
        mask = einops.rearrange(
            b["triple_mask"],
            "step shard_h ... triple -> shard_h (step ... triple)",
        )
        filtered_triples.append(reconstructed_batch[mask])

    for triple in reconstructed_batch.reshape(-1, 3):
        assert triple.tolist() in ds.triples["train"].tolist()

    if duplicate_batch:
        cutpoint = b["head"].shape[-1] // 2
        for prop in ["head", "relation", "tail"]:
            assert_equal(b[prop][..., :cutpoint], b[prop][..., cutpoint:])

    triples_all = np.sort(np.vstack(filtered_triples), axis=0)
    if duplicate_batch:
        # Check that each triple is seen twice and discard one half for final comparison
        assert_equal(triples_all[::2], triples_all[1::2])
        triples_all = triples_all[::2]
    # Check that the set of filtered triples over one epoch
    # coincides with the set of triples in dataset
    assert_equal(triples_all, np.sort(ds.triples["train"], axis=0))
