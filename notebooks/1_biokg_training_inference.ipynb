{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# KGE Training and Inference on OGBL-BioKG\n",
                "\n",
                "<em>Copyright (c) 2023 Graphcore Ltd. All rights reserved.</em>\n",
                "\n",
                "BESS-KGE (`besskge`) is a PyTorch library for knowledge graph embedding (KGE) models on IPUs implementing the distribution framework [BESS](https://arxiv.org/abs/2211.12281), with embedding tables stored in the IPU SRAM.\n",
                "\n",
                "In this notebook we will learn how to use the BESS-KGE library to train KGE models and perform link prediction inference, using the biomedical dataset [ogbl-biokg](https://ogb.stanford.edu/docs/linkprop/#ogbl-biokg)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment setup\n",
                "\n",
                "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
                "\n",
                " [![Run on Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/graphcore-research/bess-kge?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fnotebooks%2F1_biokg_training_inference.ipynb)\n",
                "\n",
                "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled and a PopTorch wheel installed. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to do this. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dependencies"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We recommend that you install `besskge` directly from the GitHub sources:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found existing installation: besskge 0.1\n",
                        "Uninstalling besskge-0.1:\n",
                        "  Successfully uninstalled besskge-0.1\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "!{sys.executable} -m pip uninstall -y besskge\n",
                "!pip install -q git+https://github.com/graphcore-research/bess-kge.git\n",
                "\n",
                "!pip install -q matplotlib"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, import the necessary dependencies. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pathlib\n",
                "import time\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import poptorch\n",
                "import torch\n",
                "\n",
                "from besskge.batch_sampler import RigidShardedBatchSampler\n",
                "from besskge.bess import EmbeddingMovingBessKGE, ScoreMovingBessKGE\n",
                "from besskge.dataset import KGDataset\n",
                "from besskge.embedding import UniformInitializer\n",
                "from besskge.loss import LogSigmoidLoss\n",
                "from besskge.metric import Evaluation\n",
                "from besskge.negative_sampler import (\n",
                "    RandomShardedNegativeSampler,\n",
                "    TripleBasedShardedNegativeSampler,\n",
                ")\n",
                "from besskge.scoring import RotatE\n",
                "from besskge.sharding import PartitionedTripleSet, Sharding\n",
                "\n",
                "dataset_directory = os.getenv(\"DATASETS_DIR\", \"../datasets/\") + \"/biokg/\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sharding entities and triples\n",
                "\n",
                "The OGBL-BioKG dataset can be downloaded and preprocessed with the built-in `KGDataset` method, `build_biokg`. `KGDataset` is the standard class which holds data from the KG dataset, such as head-relation-tail triples (with entities and relation types suitably converted to their integer IDs), triple-specific data (for example negative heads/tails to be used to corrupt the triple), ID -> label lists for entities and relation types, and so on."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of entities: 93,773\n",
                        "\n",
                        "Number of relation types: 51\n",
                        "\n",
                        "Number of triples: \n",
                        " training: 4,762,678 \n",
                        " validation/test: 162,886\n",
                        "\n",
                        "Number of negative heads/tails for validation/test triples: 500\n"
                    ]
                }
            ],
            "source": [
                "biokg = KGDataset.build_biokg(root=pathlib.Path(dataset_directory))\n",
                "\n",
                "print(f\"Number of entities: {biokg.n_entity:,}\\n\")\n",
                "print(f\"Number of relation types: {biokg.n_relation_type}\\n\")\n",
                "print(f\"Number of triples: \\n training: {biokg.triples['train'].shape[0]:,} \\n validation/test: {biokg.triples['valid'].shape[0]:,}\\n\")\n",
                "print(f\"Number of negative heads/tails for validation/test triples: {biokg.neg_heads['valid'].shape[-1]}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Entities in the OGBL-BioKG dataset have different types. Entity IDs need to be always assigned so that entities of the same type have contiguous IDs. Then, the ID offsets corresponding to different types are stored in `KGDataset.type_offsets`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'disease': 0,\n",
                            " 'drug': 10687,\n",
                            " 'function': 21220,\n",
                            " 'protein': 66305,\n",
                            " 'sideeffect': 83804}"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "biokg.type_offsets"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For example, entities with ID in the range [0, 10686] are of type 'disease'."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To train on 4 IPUs, we shard the entity set into four parts of equal size. This is done using the `Sharding` class. To use a different number of IPUs, just change the value of the `n_shard` variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of shards: 4\n",
                        "\n",
                        "Number of entities in each shard: 23,444\n",
                        "\n",
                        "Global entity IDs on 4 shards:\n",
                        " [[    0     4    12 ... 93769 93773 93774]\n",
                        " [    8    11    18 ... 93762 93771 93775]\n",
                        " [    1     2     9 ... 93761 93764 93772]\n",
                        " [    3     5     6 ... 93767 93768 93770]]\n",
                        "\n",
                        "Number of actual (=non-padding) entities per shard:\n",
                        " [23443 23443 23444 23444]\n",
                        "\n",
                        "Type offsets per shard: \n",
                        " [[    0  2689  5350 16623 20984]\n",
                        " [    0  2733  5240 16383 20862]\n",
                        " [    0  2631  5323 16657 20991]\n",
                        " [    0  2634  5307 16642 20967]]\n"
                    ]
                }
            ],
            "source": [
                "seed = 1234\n",
                "n_shard = 4\n",
                "\n",
                "sharding = Sharding.create(n_entity=biokg.n_entity, n_shard=n_shard, seed=seed, type_offsets=np.fromiter(biokg.type_offsets.values(), dtype=np.int32))\n",
                "\n",
                "print(f\"Number of shards: {sharding.n_shard}\\n\")\n",
                "\n",
                "print(f\"Number of entities in each shard: {sharding.max_entity_per_shard:,}\\n\")\n",
                "\n",
                "print(f\"Global entity IDs on {n_shard} shards:\\n {sharding.shard_and_idx_to_entity}\\n\")\n",
                "\n",
                "# If the number of entities is not divisible by n_shard, some shards will have one trailing padding entity (ID >= n_entity)\n",
                "print(f\"Number of actual (=non-padding) entities per shard:\\n {sharding.shard_counts}\\n\")\n",
                "\n",
                "# Entities of the same type maintain contiguous local IDs in each shard\n",
                "print(f\"Type offsets per shard: \\n\", sharding.entity_type_offsets)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since the total number of entities (93,773) is not divisible by `n_shard` (4), some padding entities are created. These entities have global entity IDs with values greater than or equal to 93,773 and can be seen printed above.\n",
                "\n",
                "The entity sharding induces a partitioning of the set of training triples into `n_shard ** 2 = 16` **shard-pairs**, based on the entity shard of the head and of the tail. Triple partitioning is performed using the `PartitionedTripleSet` class."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<p align=\"left\">\n",
                "<img src=\"https://raw.githubusercontent.com/graphcore-research/bess-kge/main/notebooks/img/batch.png\"  width=\"500\" >\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of triples per (h,t) shardpair:\n",
                        " [[291989 290598 314459 278671]\n",
                        " [294571 292577 314075 280268]\n",
                        " [313333 308181 331824 296695]\n",
                        " [287855 286280 307383 273919]]\n"
                    ]
                }
            ],
            "source": [
                "train_triples = PartitionedTripleSet.create_from_dataset(dataset=biokg, part=\"train\", sharding=sharding, partition_mode=\"ht_shardpair\")\n",
                "\n",
                "# train_triples.triple_counts[i,j] is the number of triples with head entity in shard i and tail entity in shard j\n",
                "print(f\"Number of triples per (h,t) shard-pair:\\n {train_triples.triple_counts}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Triples in `train_triples.triples` have been sorted based on the sequence of (h,t) shard-pairs: `(0,0), (0,1), ..., (n_shard-1, n_shard-1)`. You can use `train_triples.triple_sort_idx` to recover the original ordering.\n",
                "\n",
                "Moreover, the global entity IDs for heads and tails have been replaced with the local indices on the corresponding shard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Put original triples in the same order as train_triples.triples\n",
                "triple_sorted = biokg.triples[\"train\"][train_triples.triple_sort_idx]\n",
                "# Pass from global IDs to local IDs with sharding.entity_to_idx\n",
                "triple_sorted[:,0] = sharding.entity_to_idx[triple_sorted[:,0]]\n",
                "triple_sorted[:,2] = sharding.entity_to_idx[triple_sorted[:,2]]\n",
                "# Compare with the content of train_triples.triples\n",
                "np.all(triple_sorted == train_triples.triples)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Negative and batch samplers\n",
                "\n",
                "A key component in training KGE models is the selection of negative samples to contrast against positive triples. A standard strategy to construct negative samples is to replace (corrupt) either the head or the tail of a positive triple with another entity.\n",
                "\n",
                "To sample the negative entities used to corrupt positive triples, we use a **negative sampler**. You can find different types of negative samplers implemented in `besskge.negative_sampler`. Two suitable samplers are:\n",
                "\n",
                "* `RandomShardedNegativeSampler`, which randomly picks the negative entity among all the entities in the KG;\n",
                "* `TypeBasedShardedNegativeSampler`, which selects the corrupted entity only among entities of the same type as the original one.\n",
                "\n",
                "By default, BESS samples the same number of negative entities from each entity shard, to minimize selection bias. For each pair of devices, the same amount of negative entities is exchanged in both directions, through collective operators.\n",
                "\n",
                "* If `flat_negative_format=False`, negative entities are sampled on a triple basis. For each positive triple in a micro-batch, `n_negative` corrupted entities are received from each device, for a total of `shard_bs * n_negative * n_shard` negatives used in the micro-batch. We can then decide whether to score each of the `shard_bs` triples in the micro-batch only against the corresponding `n_negative * n_shard` negative entities, or against all negatives seen in the micro-batch (**negative sample sharing**).\n",
                "\n",
                "* If `flat_negative_format=True`, negative entities are sampled on a shard-pair basis. Each device receives `n_negative` negatives from each shard, for a total of `n_negative * n_shard` negatives used in the micro-batch. As the negatives are not sampled on a triple basis, this requires the use of negative sample sharing.\n",
                "\n",
                "In order to reduce inter-device communication, we have the option of sampling all negatives just from the device where the triple loss will be computed, by setting `local_sampling=True`. This, however, introduces a bias in the construction of negative samples.\n",
                "\n",
                "When instantiating the negative sampler, we must also specify the corruption scheme:\n",
                "* `corruption_scheme='h'` if negative samples are to be constructed by corrupting the head entity;\n",
                "* `corruption_scheme='t'` if negative samples are to be constructed by corrupting the tail entity;\n",
                "* `corruption_scheme='ht'` if negative samples are to be constructed by corrupting the head entity for half the triples in the micro-batch and the tail entity for the other half (in this case, when using negative sample sharing, negatives are shared only among the triples in the same half)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "neg_sampler = RandomShardedNegativeSampler(n_negative=1, sharding=sharding, seed=seed, corruption_scheme=\"ht\",\n",
                "                                           local_sampling=False, flat_negative_format=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In order to start training, we are missing only one component: a **batch sampler**. This class is responsible for cooking up the batches to pass to each device. This is not a trivial task, since at each step each device needs to know which embeddings stored in its local memory are needed by itself and by all other devices.\n",
                "Different types of batch samplers are implemented in `besskge.batch_sampler`. All of them, at each step, sample the same number of triples from each of the 16 shard-pair buckets. Here we use `RigidShardedBatchSampler`, where each bucket is consumed sequentially. The length of an epoch is then dictated by the length of the largest bucket."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "# triples per shardpair per step: 60 \n",
                        "\n",
                        "head         torch.Size([48, 4, 4, 60])     torch.int32;\n",
                        "relation     torch.Size([48, 4, 4, 60])     torch.int32;\n",
                        "tail         torch.Size([48, 4, 4, 60])     torch.int32;\n",
                        "triple_mask  torch.Size([48, 4, 4, 60])     torch.bool;\n",
                        "negative     torch.Size([48, 4, 4, 240, 1]) torch.int32;\n"
                    ]
                }
            ],
            "source": [
                "device_iterations = 8\n",
                "accum_factor = 6\n",
                "# Micro-batch size, which means the number of positive triples processed on each device at each step\n",
                "shard_bs = 240\n",
                "\n",
                "batch_sampler = RigidShardedBatchSampler(partitioned_triple_set=train_triples, negative_sampler=neg_sampler,\n",
                "                              shard_bs=shard_bs, batches_per_step=device_iterations*accum_factor, seed=seed)\n",
                "\n",
                "\n",
                "print(f\"# triples per shard-pair per step: {batch_sampler.positive_per_partition} \\n\")\n",
                "\n",
                "# Example batch\n",
                "idx_sampler = iter(batch_sampler.get_dataloader_sampler(shuffle=True))\n",
                "for k,v in batch_sampler[next(idx_sampler)].items():\n",
                "    print(f\"{k:<12} {str(v.shape):<30} {v.dtype};\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see that each call returns `device_iterations * accum_factor = 48` batches. Each of them is composed of 60 triples from each of the 16 shard-pair buckets. In particular, IPU `i` will process 240 triples, 60 from each of the four (h, t) shard-pairs `(i,0), (i,1), (i,2), (i,3)`. \n",
                "\n",
                "`head[:,i,:,:], tail[:,i,:,:]` are the entity IDs of the embeddings that need to be gathered from the SRAM of IPU `i`.\n",
                "\n",
                "`negative[:,i,j,t,:]` are the negative entities sampled on IPU `i` to be used for triple `t` on IPU `j` (where the trailing 1 is the `n_negative` specified in the negative sampler).\n",
                "\n",
                "`triple_mask` is a boolean mask indicating which of the positive triples in the batch are non-padding (since, as mentioned above, `RigidShardedBatchSampler` will repeat triples in smaller shard-pair buckets during an epoch). We can ignore it during training, but it is important at inference time, when we want to visit each triple only once. We can then use `triple_mask` to discard any duplicates.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<p align=\"left\">\n",
                "<img src=\"https://raw.githubusercontent.com/graphcore-research/bess-kge/main/notebooks/img/negsplit.png\"  width=\"880\" >\n",
                "</p>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Distributed training\n",
                "\n",
                "The method `get_dataloader` of the batch sampler returns the PopTorch dataloader which we iterate over during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "options = poptorch.Options()\n",
                "options.replication_factor = sharding.n_shard\n",
                "options.deviceIterations(device_iterations)\n",
                "options.Training.gradientAccumulation(accum_factor)\n",
                "# Add a memory saving optimisation pattern. This removes an unnecessary\n",
                "# entity_embedding gradient all-reduce, which is a no-op since it is fully\n",
                "# sharded across replicas.\n",
                "options._popart.setPatterns(dict(RemoveAllReducePattern=True))\n",
                "\n",
                "# Construction similar to PyTorch dataloader\n",
                "train_dl = batch_sampler.get_dataloader(options=options, shuffle=True, num_workers=5, persistent_workers=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are now ready to define the model and train it. We'll use **RotatE** with (real) 128-dimensional embeddings and the **logsigmoid** loss function with **negative adversarial sampling**. We will also use **negative sample sharing** within the micro-batches to increase the effective number of negative samples without the need for sampling and exchanging more negative entities.\n",
                "\n",
                "The **BESS distribution scheme** is implemented in `besskge.bess` and it works as a wrapper around the KGE scoring function. It comes in different flavours: here we use the basic `EmbeddingMovingBessKGE` class, where entity embeddings are exchanged between IPUs (from the one where the embedding is stored to the one where it is needed for computation) through AllToAll collectives (for details on the embedding sharing, see the [BESS-KGE documentation](https://graphcore-research.github.io/bess-kge/API_ref/bess.html))."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Graph compilation: 100%|██████████| 100/100 [00:59<00:00]\n",
                        "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n"
                    ]
                }
            ],
            "source": [
                "# Loss function\n",
                "logsigmoid_loss_fn = LogSigmoidLoss(margin=12.0, negative_adversarial_sampling=True)\n",
                "# Initialization scheme for embedding tables\n",
                "emb_initializer = UniformInitializer(range_scale=logsigmoid_loss_fn.margin)\n",
                "# KGE model\n",
                "rotate_score_fn = RotatE(negative_sample_sharing=True, scoring_norm=1, sharding=sharding,\n",
                "                  n_relation_type=biokg.n_relation_type, embedding_size=128,\n",
                "                  entity_initializer=emb_initializer, relation_initializer=emb_initializer)\n",
                "# BESS wrapper\n",
                "model = EmbeddingMovingBessKGE(negative_sampler=neg_sampler, score_fn=rotate_score_fn,\n",
                "                               loss_fn=logsigmoid_loss_fn)\n",
                "\n",
                "# Optimizer\n",
                "opt = poptorch.optim.AdamW(\n",
                "        model.parameters(),\n",
                "        lr=0.001,\n",
                "    )\n",
                "\n",
                "# PopTorch wrapper\n",
                "poptorch_model = poptorch.trainingModel(model, options=options, optimizer=opt)\n",
                "\n",
                "# The variable entity_embedding needs to hold different values on each replica,\n",
                "# corresponding to the distinct shards of the entity embedding table\n",
                "poptorch_model.entity_embedding.replicaGrouping(\n",
                "            poptorch.CommGroupType.NoGrouping,\n",
                "            0,\n",
                "            poptorch.VariableRetrievalMode.OnePerGroup,\n",
                "        )\n",
                "\n",
                "# Compile model\n",
                "batch = next(iter(train_dl))\n",
                "_ = batch.pop(\"triple_mask\")\n",
                "#  PopTorch requires to flatten device_iterations and shard into a single dimension\n",
                "res = poptorch_model(**{k: v.flatten(end_dim=1) for k, v in batch.items()})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 loss: 0.698177 --- positive triples processed: 5.35e+06\n",
                        "Epoch duration (sec): 1.95775 (average step time: 0.01533)\n",
                        "Epoch 2 loss: 0.472533 --- positive triples processed: 1.07e+07\n",
                        "Epoch duration (sec): 1.77420 (average step time: 0.01523)\n",
                        "Epoch 3 loss: 0.408885 --- positive triples processed: 1.60e+07\n",
                        "Epoch duration (sec): 1.77967 (average step time: 0.01523)\n",
                        "Epoch 4 loss: 0.386156 --- positive triples processed: 2.14e+07\n",
                        "Epoch duration (sec): 1.75322 (average step time: 0.01505)\n",
                        "Epoch 5 loss: 0.370855 --- positive triples processed: 2.67e+07\n",
                        "Epoch duration (sec): 1.74845 (average step time: 0.01502)\n",
                        "Epoch 6 loss: 0.367683 --- positive triples processed: 3.21e+07\n",
                        "Epoch duration (sec): 1.75177 (average step time: 0.01502)\n",
                        "Epoch 7 loss: 0.364207 --- positive triples processed: 3.74e+07\n",
                        "Epoch duration (sec): 1.75988 (average step time: 0.01511)\n",
                        "Epoch 8 loss: 0.360826 --- positive triples processed: 4.28e+07\n",
                        "Epoch duration (sec): 1.75050 (average step time: 0.01502)\n",
                        "Epoch 9 loss: 0.360428 --- positive triples processed: 4.81e+07\n",
                        "Epoch duration (sec): 1.75207 (average step time: 0.01505)\n",
                        "Epoch 10 loss: 0.357067 --- positive triples processed: 5.35e+07\n",
                        "Epoch duration (sec): 1.77210 (average step time: 0.01521)\n",
                        "Epoch 11 loss: 0.354295 --- positive triples processed: 5.88e+07\n",
                        "Epoch duration (sec): 1.75505 (average step time: 0.01504)\n",
                        "Epoch 12 loss: 0.354797 --- positive triples processed: 6.41e+07\n",
                        "Epoch duration (sec): 1.78257 (average step time: 0.01529)\n",
                        "Epoch 13 loss: 0.356201 --- positive triples processed: 6.95e+07\n",
                        "Epoch duration (sec): 1.76318 (average step time: 0.01513)\n",
                        "Epoch 14 loss: 0.355377 --- positive triples processed: 7.48e+07\n",
                        "Epoch duration (sec): 1.75741 (average step time: 0.01508)\n",
                        "Epoch 15 loss: 0.354679 --- positive triples processed: 8.02e+07\n",
                        "Epoch duration (sec): 1.76560 (average step time: 0.01514)\n",
                        "Epoch 16 loss: 0.354824 --- positive triples processed: 8.55e+07\n",
                        "Epoch duration (sec): 1.75487 (average step time: 0.01507)\n",
                        "Epoch 17 loss: 0.355270 --- positive triples processed: 9.09e+07\n",
                        "Epoch duration (sec): 1.75812 (average step time: 0.01509)\n",
                        "Epoch 18 loss: 0.353315 --- positive triples processed: 9.62e+07\n",
                        "Epoch duration (sec): 1.76335 (average step time: 0.01514)\n",
                        "Epoch 19 loss: 0.352730 --- positive triples processed: 1.02e+08\n",
                        "Epoch duration (sec): 1.74504 (average step time: 0.01499)\n",
                        "Epoch 20 loss: 0.351751 --- positive triples processed: 1.07e+08\n",
                        "Epoch duration (sec): 1.76846 (average step time: 0.01518)\n",
                        "Epoch 21 loss: 0.352701 --- positive triples processed: 1.12e+08\n",
                        "Epoch duration (sec): 1.74792 (average step time: 0.01501)\n",
                        "Epoch 22 loss: 0.353487 --- positive triples processed: 1.18e+08\n",
                        "Epoch duration (sec): 1.76492 (average step time: 0.01513)\n",
                        "Epoch 23 loss: 0.350079 --- positive triples processed: 1.23e+08\n",
                        "Epoch duration (sec): 1.75427 (average step time: 0.01505)\n",
                        "Epoch 24 loss: 0.352689 --- positive triples processed: 1.28e+08\n",
                        "Epoch duration (sec): 1.74967 (average step time: 0.01502)\n",
                        "Epoch 25 loss: 0.354066 --- positive triples processed: 1.34e+08\n",
                        "Epoch duration (sec): 1.77196 (average step time: 0.01517)\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Train for 25 epochs\n",
                "n_epochs = 25\n",
                "\n",
                "# Positive triples seen until now\n",
                "cumulative_triples = 0\n",
                "training_loss = []\n",
                "for ep in range(n_epochs):\n",
                "    ep_start_time = time.time()\n",
                "    ep_log = []\n",
                "    for batch in train_dl:\n",
                "        step_start_time = time.time()\n",
                "        # triple_mask is not used during training\n",
                "        triple_mask = batch.pop(\"triple_mask\")\n",
                "        cumulative_triples += triple_mask.numel()\n",
                "        res = poptorch_model(**{k: v.flatten(end_dim=1) for k, v in batch.items()})\n",
                "        # res[\"loss\"] contains the summed loss of elements in the last batch, for each IPU\n",
                "        ep_log.append(dict(loss=float(torch.sum(res[\"loss\"])) / triple_mask[-1].numel(), step_time=(time.time()-step_start_time)))\n",
                "    ep_loss = [v['loss'] for v in ep_log]\n",
                "    training_loss.extend([v['loss'] for v in ep_log])\n",
                "    print(f\"Epoch {ep+1} loss: {np.mean(ep_loss):.6f} --- positive triples processed: {cumulative_triples:.2e}\")\n",
                "    print(f\"Epoch duration (sec): {(time.time() - ep_start_time):.5f} (average step time: {np.mean([v['step_time'] for v in ep_log]):.5f})\")\n",
                "\n",
                "# Plot loss as a function of the number of positive triples processed\n",
                "total_triples = np.cumsum(n_epochs * len(train_dl) * [triple_mask.numel()])\n",
                "ax = plt.gca()\n",
                "ax.plot(total_triples, training_loss)\n",
                "ax.set_xscale(\"log\")\n",
                "ax.set_xlabel(\"Positive triples\")\n",
                "ax.set_ylabel(\"Loss\")\n",
                "\n",
                "poptorch_model.detachFromDevice()\n",
                "del train_dl"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inference\n",
                "\n",
                "Let's see now how the trained model performs on the **validation** set. \n",
                "\n",
                "We create a new `PartitionedTripleSet` holding the validation triples. Differently from the training set, each validation triple has a specific set of 500 negative heads/tails to be scored against, hence we use the `TripleBasedShardedNegativeSampler` negative sampler class to sample exactly the negative entities needed by each triple. This means that we will not be using negative sample sharing any more.\n",
                "\n",
                "For the batch sampler, we again use the `RigidShardedBatchSampler` class, but we now set the option `duplicate_batch=True`. This means that the two halves of the micro-batch (where we corrupt heads and tails respectively) contain the same positive triples, so that we can score negative heads and negative tails with a single model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "head            torch.Size([10, 4, 4, 60])          torch.int32;\n",
                        "relation        torch.Size([10, 4, 4, 60])          torch.int32;\n",
                        "tail            torch.Size([10, 4, 4, 60])          torch.int32;\n",
                        "triple_mask     torch.Size([10, 4, 4, 60])          torch.bool;\n",
                        "negative        torch.Size([10, 4, 4, 240, 175])    torch.int32;\n",
                        "negative_mask   torch.Size([10, 4, 240, 4, 175])    torch.bool;\n"
                    ]
                }
            ],
            "source": [
                "valid_triples = PartitionedTripleSet.create_from_dataset(dataset=biokg, part=\"valid\", sharding=sharding, partition_mode=\"ht_shardpair\")\n",
                "ns_valid = TripleBasedShardedNegativeSampler(negative_heads=valid_triples.neg_heads, negative_tails=valid_triples.neg_tails,\n",
                "                                             sharding=sharding, corruption_scheme=\"ht\", seed=seed)\n",
                "bs_valid = RigidShardedBatchSampler(partitioned_triple_set=valid_triples, negative_sampler=ns_valid, shard_bs=shard_bs, batches_per_step=10,\n",
                "                                    seed=seed, duplicate_batch=True)\n",
                "\n",
                "# Example batch\n",
                "idx_sampler = iter(bs_valid.get_dataloader_sampler(shuffle=False))\n",
                "for k,v in bs_valid[next(idx_sampler)].items():\n",
                "    print(f\"{k:<15} {str(v.shape):<35} {v.dtype};\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see that the `negative` tensor returned now by the dataloader has a trailing dimension of 175, meaning that each validation query is scored against `4*175` negative heads/tails, which is larger than 500. This is due to the fact that the triple-specific negatives are, in general, not equally split between the 4 shards, therefore some padding needs to be applied. `negative_mask` returned by the negative sampler is used to identify the padding negative entities, so that the corresponding scores can be filtered out when computing the metrics.\n",
                "\n",
                "We can now instantiate the inference model. We use the `besskge.metric.Evaluation` class to specify which **metrics** we want to compute and pass it to the BESS module. Here we look at Hits@K for K=1,5,10, giving us the percentage of the validation triples where the ground truth head/tail is among the K most-likely predictions made by the model, and the mean reciprocal rank (MRR). For all these metrics, **higher is better**. By specifying `reduction=\"sum\"` we reduce communication between host and device, by returning the summed values, over the elements in the same micro-batch, for each metric.\n",
                "\n",
                "We now use a different flavour of BESS compared to training, namely `ScoreMovingBessKGE`. This is recommended when the number of negative entities to be fetched from other devices is large, as it is typically the case when using `TripleBasedShardedNegativeSampler`. While `EmbeddingMovingBessKGE` sends the negative embeddings to the device where the positive triple is scored, `ScoreMovingBessKGE` fetches the queries with an AllGather and computes negative scores on the shard where the negative entities are stored, and then sends the scores back to the original device. This allows for communicating of scores instead of embeddings, which is usually cheaper, although it requires additional collective communications between devices. We encourage you to play with different configurations to see which one gives the shorter overall validation time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[11:29:42.997] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 435\n",
                        "[11:29:43.009] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 441\n",
                        "Graph compilation: 100%|██████████| 100/100 [00:56<00:00]\n",
                        "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n"
                    ]
                }
            ],
            "source": [
                "# Each triple is now to be scored against a specific set of negatives, so we turn off negative sample sharing\n",
                "rotate_score_fn.negative_sample_sharing = False\n",
                "\n",
                "val_options = poptorch.Options()\n",
                "val_options.replication_factor = sharding.n_shard\n",
                "val_options.deviceIterations(bs_valid.batches_per_step)\n",
                "val_options.outputMode(poptorch.OutputMode.All)\n",
                "\n",
                "# Validation dataloader\n",
                "valid_dl = bs_valid.get_dataloader(options=val_options, shuffle=False, num_workers=3, persistent_workers=True)\n",
                "\n",
                "# With reduction=\"sum\" the returned res[\"metrics\"] has shape (batches_per_step * n_shard, n_metrics)\n",
                "evaluator = Evaluation([\"mrr\", \"hits@1\", \"hits@5\", \"hits@10\"], reduction=\"sum\")\n",
                "# BESS wrapper\n",
                "model_inf = ScoreMovingBessKGE(negative_sampler=ns_valid, score_fn=rotate_score_fn, evaluation=evaluator)\n",
                "\n",
                "# PopTorch wrapper\n",
                "poptorch_model_inf = poptorch.inferenceModel(model_inf, options=val_options)\n",
                "poptorch_model_inf.entity_embedding.replicaGrouping(\n",
                "            poptorch.CommGroupType.NoGrouping,\n",
                "            0,\n",
                "            poptorch.VariableRetrievalMode.OnePerGroup,\n",
                "        )\n",
                "\n",
                "# Compile model\n",
                "batch = next(iter(valid_dl))\n",
                "res = poptorch_model_inf(**{k: v.flatten(end_dim=1) for k, v in batch.items()})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "hits@1 : 0.745279\n",
                        "hits@5 : 0.905296\n",
                        "hits@10 : 0.941364\n",
                        "mrr : 0.816847\n"
                    ]
                }
            ],
            "source": [
                "# Perform validation and print metrics\n",
                "\n",
                "val_log = []\n",
                "# The final value of n_val_queries will be twice the number of triples in the validation set\n",
                "# as each triple is scored against negative heads and negative tails separately\n",
                "n_val_queries = 0\n",
                "for batch_val in valid_dl:\n",
                "    res = poptorch_model_inf(**{k: v.flatten(end_dim=1) for k, v in batch_val.items()})\n",
                "    \n",
                "    n_val_queries += batch_val[\"triple_mask\"].sum()\n",
                "    # By transposing res[\"metrics\"] we separate the outputs for the different metrics\n",
                "    val_log.append({k: v.sum() for k, v in zip(\n",
                "                        evaluator.metrics.keys(),\n",
                "                        res[\"metrics\"].T,\n",
                "                    )})\n",
                "\n",
                "for metric in val_log[0].keys():\n",
                "    reduced_metric = sum([l[metric] for l in val_log]) / n_val_queries\n",
                "    print(\"%s : %f\" % (metric, reduced_metric))\n",
                "\n",
                "poptorch_model_inf.detachFromDevice()\n",
                "del valid_dl"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## How many parameters does our model have?\n",
                "\n",
                "We can see how many parameters the model has:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "# model parameters: 12,006,592\n"
                    ]
                }
            ],
            "source": [
                "# Equivalent to sum(p.numel() for p in model_inf.parameters()), as embeddings are the only trainable parameters\n",
                "print(f\"# model parameters: {model_inf.n_embedding_parameters:,}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Looking at the [OGB leaderboard](https://ogb.stanford.edu/docs/leader_linkprop/#ogbl-biokg) for the OGBL-BioKG dataset, the validation MRR we got is **very competitive even when compared against other classical shallow KGE models with 15 times more parameters** than we used. Not bad for a model we trained so quickly!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Scaling analysis\n",
                "\n",
                "The OGBL-BioKG knowledge graph is small enough that we can actually train the model above (with same embedding size) on a single IPU. This allows us to get a better grasp on the advantages of distribution.\n",
                "\n",
                "In order to train on one IPU, we just need to set `n_shard = 1` when defining the `Sharding` of the entity table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of shards: 1\n",
                        "\n",
                        "Number of entities in each shard: 93773\n",
                        "\n",
                        "Number of triples per (h,t) shardpair:\n",
                        " [[4762678]]\n"
                    ]
                }
            ],
            "source": [
                "n_shard = 1\n",
                "\n",
                "sharding = Sharding.create(n_entity=biokg.n_entity, n_shard=n_shard, seed=seed, type_offsets=np.fromiter(biokg.type_offsets.values(), dtype=np.int32))\n",
                "\n",
                "print(f\"Number of shards: {sharding.n_shard}\\n\")\n",
                "\n",
                "# All entities in the KG are now on a single shard\n",
                "print(f\"Number of entities in each shard: {sharding.max_entity_per_shard}\\n\")\n",
                "\n",
                "train_triples = PartitionedTripleSet.create_from_dataset(dataset=biokg, part=\"train\", sharding=sharding, partition_mode=\"ht_shardpair\")\n",
                "\n",
                "# There is now a single (h,t) shard-pair, containing all training triples \n",
                "print(f\"Number of triples per (h,t) shard-pair:\\n {train_triples.triple_counts}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As explained in the section [Negative and batch samplers](#negative-and-batch-samplers), in order to score each triple against the same number of negatives as before, we need to multiply `n_negative` by 4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reducing the number of shards by a factor of 4, the number of negatives per triple per shard (i.e. n_negative) needs to increase by the same factor\n",
                "neg_sampler = RandomShardedNegativeSampler(n_negative=4, sharding=sharding, seed=seed, corruption_scheme=\"ht\",\n",
                "                                           local_sampling=False, flat_negative_format=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Similarly, since we maintain the same micro-batch size used before (to get a fair comparison using negative sample sharing), we multiply the accumulation factor by 4 so that the global batch size is the same."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "# triples per shardpair per step: 240 \n",
                        "\n",
                        "head         torch.Size([192, 1, 1, 240])   torch.int32;\n",
                        "relation     torch.Size([192, 1, 1, 240])   torch.int32;\n",
                        "tail         torch.Size([192, 1, 1, 240])   torch.int32;\n",
                        "triple_mask  torch.Size([192, 1, 1, 240])   torch.bool;\n",
                        "negative     torch.Size([192, 1, 1, 240, 4]) torch.int32;\n"
                    ]
                }
            ],
            "source": [
                "device_iterations = 8\n",
                "accum_factor = 24\n",
                "shard_bs = 240\n",
                "\n",
                "batch_sampler = RigidShardedBatchSampler(partitioned_triple_set=train_triples, negative_sampler=neg_sampler,\n",
                "                              shard_bs=shard_bs, batches_per_step=device_iterations*accum_factor, seed=seed)\n",
                "\n",
                "\n",
                "print(f\"# triples per shard-pair per step: {batch_sampler.positive_per_partition} \\n\")\n",
                "\n",
                "# Example batch\n",
                "idx_sampler = iter(batch_sampler.get_dataloader_sampler(shuffle=True))\n",
                "for k,v in batch_sampler[next(idx_sampler)].items():\n",
                "    print(f\"{k:<12} {str(v.shape):<30} {v.dtype};\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The dataloader and model are created exactly as before."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Graph compilation: 100%|██████████| 100/100 [01:01<00:00]\n"
                    ]
                }
            ],
            "source": [
                "options = poptorch.Options()\n",
                "options.deviceIterations(device_iterations)\n",
                "options.Training.gradientAccumulation(accum_factor)\n",
                "\n",
                "train_dl = batch_sampler.get_dataloader(options=options, shuffle=True, num_workers=5, persistent_workers=True)\n",
                "\n",
                "logsigmoid_loss_fn = LogSigmoidLoss(margin=12.0, negative_adversarial_sampling=True)\n",
                "emb_initializer = UniformInitializer(range_scale=logsigmoid_loss_fn.margin)\n",
                "rotate_score_fn = RotatE(negative_sample_sharing=True, scoring_norm=1, sharding=sharding,\n",
                "                  n_relation_type=biokg.n_relation_type, embedding_size=128,\n",
                "                  entity_initializer=emb_initializer, relation_initializer=emb_initializer)\n",
                "\n",
                "model = EmbeddingMovingBessKGE(negative_sampler=neg_sampler, score_fn=rotate_score_fn,\n",
                "                               loss_fn=logsigmoid_loss_fn)\n",
                "\n",
                "opt = poptorch.optim.AdamW(\n",
                "        model.parameters(),\n",
                "        lr=0.001,\n",
                "    )\n",
                "\n",
                "poptorch_model = poptorch.trainingModel(model, options=options, optimizer=opt)\n",
                "poptorch_model.entity_embedding.replicaGrouping(\n",
                "            poptorch.CommGroupType.NoGrouping,\n",
                "            0,\n",
                "            poptorch.VariableRetrievalMode.OnePerGroup,\n",
                "        )\n",
                "\n",
                "\n",
                "# Compile model\n",
                "batch = next(iter(train_dl))\n",
                "_ = batch.pop(\"triple_mask\")\n",
                "res = poptorch_model(**{k: v.flatten(end_dim=1) for k, v in batch.items()})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 loss: 0.711565 --- positive triples processed: 4.79e+06\n",
                        "Epoch duration (sec): 5.97384 (average step time: 0.05022)\n",
                        "Epoch 2 loss: 0.497848 --- positive triples processed: 9.58e+06\n",
                        "Epoch duration (sec): 5.20361 (average step time: 0.04997)\n",
                        "Epoch 3 loss: 0.423704 --- positive triples processed: 1.44e+07\n",
                        "Epoch duration (sec): 5.19094 (average step time: 0.04985)\n",
                        "Epoch 4 loss: 0.390434 --- positive triples processed: 1.92e+07\n",
                        "Epoch duration (sec): 5.18438 (average step time: 0.04979)\n",
                        "Epoch 5 loss: 0.380970 --- positive triples processed: 2.40e+07\n",
                        "Epoch duration (sec): 5.20511 (average step time: 0.04998)\n",
                        "Epoch 6 loss: 0.371948 --- positive triples processed: 2.88e+07\n",
                        "Epoch duration (sec): 5.19947 (average step time: 0.04992)\n",
                        "Epoch 7 loss: 0.371240 --- positive triples processed: 3.35e+07\n",
                        "Epoch duration (sec): 5.19853 (average step time: 0.04992)\n",
                        "Epoch 8 loss: 0.362567 --- positive triples processed: 3.83e+07\n",
                        "Epoch duration (sec): 5.19344 (average step time: 0.04987)\n",
                        "Epoch 9 loss: 0.364435 --- positive triples processed: 4.31e+07\n",
                        "Epoch duration (sec): 5.19695 (average step time: 0.04991)\n",
                        "Epoch 10 loss: 0.361335 --- positive triples processed: 4.79e+07\n",
                        "Epoch duration (sec): 5.23683 (average step time: 0.05027)\n"
                    ]
                }
            ],
            "source": [
                "# Train for the first 10 epochs\n",
                "n_epochs = 10\n",
                "\n",
                "cumulative_triples = 0\n",
                "training_loss = []\n",
                "for ep in range(n_epochs):\n",
                "    ep_start_time = time.time()\n",
                "    ep_log = []\n",
                "    for batch in train_dl:\n",
                "        step_start_time = time.time()\n",
                "        triple_mask = batch.pop(\"triple_mask\")\n",
                "        cumulative_triples += triple_mask.numel()\n",
                "        res = poptorch_model(**{k: v.flatten(end_dim=1) for k, v in batch.items()})\n",
                "        ep_log.append(dict(loss=float(torch.sum(res[\"loss\"])) / triple_mask[-1].numel(), step_time=(time.time()-step_start_time)))\n",
                "    ep_loss = [v['loss'] for v in ep_log]\n",
                "    training_loss.extend([v['loss'] for v in ep_log])\n",
                "    print(f\"Epoch {ep+1} loss: {np.mean(ep_loss):.6f} --- positive triples processed: {cumulative_triples:.2e}\")\n",
                "    print(f\"Epoch duration (sec): {(time.time() - ep_start_time):.5f} (average step time: {np.mean([v['step_time'] for v in ep_log]):.5f})\")\n",
                "\n",
                "poptorch_model.detachFromDevice()\n",
                "del train_dl"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "By comparing the epoch durations, we see that **scaling from 1 to 4 IPUs speeds up training by a factor of ~3X**. This is actually a conservative estimate, because - as one can notice from the printouts - the number of triples processed at each epoch is larger when running on 4 IPUs, as a consequence of using `RigidShardedBatchSampler` (the number of triples seen during an epoch is determined by the number of triples in the largest shard-pair bucket)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusions and next steps\n",
                "\n",
                "To recap, these are the basic steps to run distributed training/inference with BESS-KGE:\n",
                "* wrap your KG dataset with `besskge.dataset.KGDataset`;\n",
                "* shard entities in the graph based on the number of IPUs you want to use, by using `besskge.sharding.Sharding`, and partition triples accordingly with `besskge.sharding.PartitionedTripleSet`;\n",
                "* select a negative sampler from `besskge.negative_sampler` to sample the entities used to corrupt positive triples;\n",
                "* use a batch sampler from `besskge.batch_sampler` to create the batch dataloader; \n",
                "* specify the scoring and loss function (see `besskge.scoring` and `besskge.loss`) and initialize the embedding tables, either from an existing checkpoint or with one of the initialization schemes from `besskge.embedding.EmbeddingInitializer`;\n",
                "* instantiate the distributed model with one of the subclasses of `besskge.bess.BessKGE`.\n",
                "\n",
                "You can easily modify this notebook to use different combinations of KGE models, embedding sizes and loss functions, change the number of negative samples or the sampling scheme (for instance, try training with `TypeBasedShardedNegativeSampler`).\n",
                "\n",
                "What next? If you are interested in using these models to complete queries when no candidate heads/tails are provided, have a look at the the [Knowledge Graph Completion on YAGO3-10](2_yago_topk_prediction.ipynb) notebook. For using FP16 embeddings to deal with larger graphs, look at the [FP16 embeddings on OGBL-WikiKG2](3_wikikg2_fp16.ipynb) notebook."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv_3.2",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
